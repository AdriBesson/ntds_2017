{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTDS assignment 3: spectral graph theory\n",
    "[MichaÃ«l Defferrard](http://deff.ch), *PhD student*, [EPFL](http://epfl.ch) [LTS2](http://lts2.epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two assignments were designed to warm you up. This third assignment is closer to what you'll have to do for the projects. It only misses the exploratory data analysis part (we'll do that later as an exercise). As such, this exercises is composed of two parts:\n",
    "1. Data collection,\n",
    "2. Data exploitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data collection from the FMA\n",
    "\n",
    "In the first part of the assignment, we are going to collect some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv('../data/fma_tracks.csv', index_col=0)\n",
    "genres = pd.read_csv('../data/fma_genres.csv', index_col=0)\n",
    "features = pd.read_csv('../data/fma_features.csv', index_col=0, header=[0, 1, 2])\n",
    "\n",
    "#tracks.drop(20366, inplace=True)\n",
    "#features.drop(20366, inplace=True)\n",
    "\n",
    "#tracks = tracks[:1000]\n",
    "#features = features[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre1 = tracks['genre'] == 1235  # Instrumental\n",
    "genre2 = tracks['genre'] == 21    # Hip-Hop\n",
    "\n",
    "features = features.loc[genre1 | genre2, 'mfcc']\n",
    "genres = tracks.loc[genre1 | genre2, 'genre']\n",
    "\n",
    "features.shape, genres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(genre1), sum(genre2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen to the music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Feature extraction\n",
    "\n",
    "As is often the case, the data at hand is too large to be dealt with directly. We have to represent it with a smaller set of features, chosen to be maximally relevant to the task. (Manual feature extraction can sometimes be replaced by end-to-end learning systems.)\n",
    "\n",
    "For music, MFCC are often relevant spectral features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features -= features.mean(0)\n",
    "features /= features.std(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Graph construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is the graph connected?\n",
    "* Shall we use the un-normalized or normalized Laplacian? Choose and justify.\n",
    "\n",
    "Compute the l2. Or choose.\n",
    "\n",
    "Hints:\n",
    "* Use the `distance.pdist()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Compute distances\n",
    "\n",
    "Metric\n",
    "\n",
    "The Euclidean distance is defined as $$d(i,j) = \\|x_i - x_j\\|_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "distances = distance.pdist(features, metric='euclidean')\n",
    "distances = distance.squareform(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distances.reshape(-1), bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are some distances equal to zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} distances equal exactly zero. Why?'.format(np.sum(distances == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compute the weight matrix\n",
    "\n",
    "Gaussian kernel $$\\mathbf{W}(i,j) = \\exp \\left( \\frac{-d^2(i, j)}{\\sigma^2} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_width = distances.mean()\n",
    "weights = np.exp(-distances**2 / kernel_width**2)\n",
    "\n",
    "np.fill_diagonal(weights, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of graph is that? Fully connected.\n",
    "\n",
    "Sparsify the graph. Either knn or $\\epsilon$. knn better to enforce connectedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(2, 2, figsize=(17, 8))\n",
    "def plot(weights, axes):\n",
    "    axes[0].spy(weights)\n",
    "    axes[1].hist(weights[weights > 0].reshape(-1), bins=50);\n",
    "plot(weights, axes[:, 0])\n",
    "\n",
    "if False:\n",
    "    epsilon = np.percentile(weights, 80)\n",
    "    weights[weights < epsilon] = 0\n",
    "else:\n",
    "    NEIGHBORS = 10\n",
    "    idx = np.argsort(weights)[:, :-NEIGHBORS]\n",
    "    for i in range(weights.shape[0]):\n",
    "        weights[i, idx[i, :]] = 0\n",
    "    weights = np.maximum(weights, weights.T)\n",
    "\n",
    "plot(weights, axes[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Compute the Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = weights.sum(0)\n",
    "\n",
    "plt.hist(degrees, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinatorial Laplacian.\n",
    "laplacian = np.diag(degrees) - weights\n",
    "\n",
    "# Normalized Laplacian.\n",
    "deg_inv = np.diag(1 / np.sqrt(degrees))\n",
    "laplacian = deg_inv @ laplacian @ deg_inv\n",
    "\n",
    "# Alternatively:\n",
    "# laplacian = np.identity(weights.shape[0]) - deg_inv @ weights @ deg_inv\n",
    "\n",
    "plt.spy(laplacian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian = sparse.csr_matrix(laplacian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many edges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} edges out of {} x {} = {}'.format(laplacian.nnz, *weights.shape, weights.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Bonus\n",
    "\n",
    "Can you think of a way to observe if the two genres form clusters in the graph we created?\n",
    "\n",
    "Hint: Use only the weight matrix / laplacian and the labels.\n",
    "\n",
    "Sort the rows and columns given the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Eigenvectors & eigenvalues\n",
    "\n",
    "No need to compute the Fourier basis, only the Fiedler vector, i.e. the eigenvector associated to $\\lambda_2$.\n",
    "\n",
    "Use one of the following functions: `np.linalg.eig`, `np.linalg.eigh`, `sparse.linalg.eigs`, `sparse.linalg.eigsh`. Justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = sparse.linalg.eigsh(laplacian, k=10, which='SM')\n",
    "\n",
    "# That's much slower:\n",
    "# eigenvalues, eigenvectors = np.linalg.eigh(laplacian.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eigenvalues, '.-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the graph connected? Justify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you expect as the result of the below computation? Justify. Do you get the value you expected? If not, why?\n",
    "\n",
    "Note that `x @ y` is equivalent to `np.matmul(x, y)`. You should prefer the former as it makes it easier to read formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(laplacian @ eigenvectors[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.** We expect zero because the first eigenvalue is zero. The small error is due to numerical precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare different techniques.\n",
    "PCA, Fiedler, spectral clustering\n",
    "\n",
    "Visualization with Laplacian eigenmaps\n",
    "\n",
    "Principal component analysis (PCA), no graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition\n",
    "\n",
    "features_pca = skl.decomposition.PCA(n_components=2).fit_transform(features)\n",
    "\n",
    "genres = skl.preprocessing.LabelEncoder().fit_transform(genres)\n",
    "\n",
    "plt.scatter(features_pca[:,0], features_pca[:,1], c=genres, cmap='RdBu', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(eigenvectors[:, 1], eigenvectors[:, 2], c=genres, cmap='RdBu', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the tracks with the Fiedler vector. How many tracks were wrongly identified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (eigenvectors[:, 1] > 0)\n",
    "\n",
    "plt.scatter(eigenvectors[:, 1], eigenvectors[:, 2], c=labels, cmap='RdBu', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.sum(np.abs(labels - genres))\n",
    "err = err if err < len(labels)/2 else len(labels) - err\n",
    "print('{} errors ({}%)'.format(err, err/len(labels)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune some parameters (e.g. `kernel_width`, `NEIGHBORS`) to get less errors. You should get an error rate lower than 15% (i.e. less than 300 errors in total). Try to understand the effect of each parameter."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
