{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTDS assignment 3: spectral graph theory\n",
    "[MichaÃ«l Defferrard](http://deff.ch), *PhD student*, [EPFL](http://epfl.ch) [LTS2](http://lts2.epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two assignments were designed to warm you up. This third assignment is closer to what you'll have to do for the projects. It only misses the exploratory data analysis part (we'll do that later as an exercise). As such, this exercises is composed of two parts:\n",
    "1. Data collection,\n",
    "2. Data exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our plan for the assignment:\n",
    "1. Data collection: use a web API to collect the musical genre of 2000 songs.\n",
    "2. Feature extraction: compute features from audio tracks.\n",
    "3. Graph construction: construct a nearest-neighbor graph from the features.\n",
    "4. Eigendecomposition: factorization of the graph Laplacian (c.f. spectral graph theory).\n",
    "5. Visualization & Clustering: using the graph and eigenvectors to visualize the dataset and cluster the tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse, stats, spatial\n",
    "import scipy.sparse.linalg\n",
    "from sklearn import preprocessing, decomposition\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above cell fails, it's most probably because you miss a package. Install it with e.g. `conda install librosa` or `pip install librosa`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data collection\n",
    "\n",
    "Like in any data project, the first part of the assignment is to collect some data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the genre of a single track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As often, we need an API key for certain operations. Add the following to your `credentials.ini` file. I gave a key during the lab on November 6. If you were not there, ask one of your classmates.\n",
    "```\n",
    "[freemusicarchive]\n",
    "api_key = MY-KEY\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the confidential api key.\n",
    "credentials = configparser.ConfigParser()\n",
    "credentials.read(os.path.join('..', 'credentials.ini'))\n",
    "api_key = credentials.get('freemusicarchive', 'api_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to develop a function to retrieve the genre ID of a track given its track ID using the [FMA API](https://freemusicarchive.org/api).\n",
    "\n",
    "Hints:\n",
    "* A track might have multiple genres associated to it. Always return the first one and discard the others.\n",
    "    * Note: you should never discard data blindly. I selected the tracks so that this is not a problem.\n",
    "* The `get_genre` function takes an integer as input, the track ID, and returns another integer, the genre ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genre(track_id):\n",
    "    \"\"\"Returns the genre of a track by querying the API.\"\"\"\n",
    "    BASE_URL = 'https://freemusicarchive.org/api/get/tracks.json'\n",
    "    url = '{}?track_id={}&api_key={}'.format(BASE_URL, track_id, api_key)\n",
    "    response = requests.get(url).json()\n",
    "    return int(response['dataset'][0]['track_genres'][0]['genre_id'])\n",
    "\n",
    "# A correct implementation should pass the below test.\n",
    "assert get_genre(1219) == 38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create a table of tracks\n",
    "\n",
    "The `fma_tracks.csv` file contains a list of 2'000 track IDs that we will use through this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv('../data/fma_tracks.csv', index_col=0)\n",
    "print('You got {} track IDs.'.format(len(tracks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once imported by pandas, each row of the DataFrame represents a track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Add the genre to the table\n",
    "\n",
    "Your task is to add a `genre` column to the above created `tracks` DataFrame. The column should contain an integer which represents the genre of the track, i.e. the return value of the `get_genre` function you developed.\n",
    "\n",
    "Hints:\n",
    "* When developing, retrieve the genre of the first 10 tracks only. Only once your code works run it through all the tracks. That will save you time.\n",
    "* As we want to apply one function (`get_genre`) to many data samples (the 2000 track IDs), you may want to use a functional approach. Check out `tracks.apply()` or the built-in `map`. In Python, you can declare an [anonymous function](https://en.wikipedia.org/wiki/Anonymous_function) as `lambda x: x + 1`.\n",
    "* Your table should look like the below table, except with the correct number instead of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks['genre'] = 0\n",
    "tracks.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = tracks[:10]\n",
    "\n",
    "#tracks['genre'] = tracks.apply(lambda track: get_genre(track.name), axis=1)\n",
    "\n",
    "# Alternatively:\n",
    "# tracks['genre'] = list(map(get_genre, tracks.index))\n",
    "\n",
    "# Alternatively:\n",
    "# for tid in tqdm(tracks.index[:10]):\n",
    "#     tracks.at[tid, 'genre'] = get_genre(tid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Save the data\n",
    "\n",
    "To avoid having to collect the data everytime you restart the IPython kernel, save the DataFrame as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tracks.to_csv('../data/fma_tracks_with_genre.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now load it back with the following call instead of running the code in sections 1.1 to 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv('../data/fma_tracks_with_genre.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Data cleaning\n",
    "\n",
    "As always, data cleaning is necessary when dealing with real (as opposed to synthetic) data. In this case, we only need to \"summarize the genres\". The tracks I've selected for the assignment belong to either one of the following *top-level genres*: Rock (`genre_id=12`) and Hip-Hop (`genre_id=21`). There *actual genre(s)* might however be more specific and be a sub-genre of those. For example Punk is a sub-genre of Rock. You can explore the genre hierarchy on the [Free Music Archive](http://freemusicarchive.org/genre/Rock/). The below function will return the correct top-level genre for any of the sub-genres you'll encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_genre(genre_id):\n",
    "    return 21 if genre_id in [21, 83, 100, 539, 542, 811] else 12\n",
    "\n",
    "tracks = tracks.applymap(lambda genre: get_top_genre(genre))\n",
    "tracks.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went fine, you should now have 1000 Rock (`genre_id=12`) and 1000 Hip-Hop (`genre_id=12`) tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Feature extraction\n",
    "\n",
    "As is often the case, the data at hand is too large to be dealt with directly. We have to represent it with a smaller set of features, chosen to be maximally relevant to the task. (Manual feature extraction can sometimes be replaced by end-to-end learning systems.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Get raw data\n",
    "\n",
    "Decompress the zip you found on Moodle and write below the path to the mp3 files. The zip contains a 30s excerpt for each track listed in the `tracks` table. The name of the audio file is simply the track ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, let's listen to some music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(track_id):\n",
    "    return os.path.join('..', 'data', '{:06d}.mp3'.format(track_id))\n",
    "\n",
    "# 1. Get the path to the file.\n",
    "filepath = get_path(tracks.index[0])\n",
    "print('File: {}'.format(filepath))\n",
    "\n",
    "# 2. Decode the mp3 and load the audio in memory.\n",
    "audio, sampling_rate = librosa.load(filepath, sr=None, mono=True)\n",
    "print('Duration: {:.2f}s, {} samples'.format(audio.shape[-1] / sampling_rate, audio.size))\n",
    "\n",
    "# 3. Load the audio in the browser and play it.\n",
    "start, end = 7, 17\n",
    "ipd.Audio(data=audio[start*sampling_rate:end*sampling_rate], rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Spectral features\n",
    "\n",
    "For music, the [mel-frequency cepstral coefficients (MFCCs)](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) are often relevant spectral features. The parameter here (except the choice of feature), is the number of coefficients to compute.\n",
    "\n",
    "Hint:\n",
    "* Use the function `librosa.feature.mfcc` to compute those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MFCC = 20\n",
    "\n",
    "def compute_mfcc(track_id):\n",
    "    audio, sampling_rate = librosa.load(get_path(track_id), sr=None, mono=True)\n",
    "    return librosa.feature.mfcc(y=audio, sr=sampling_rate, n_mfcc=N_MFCC)\n",
    "\n",
    "assert compute_mfcc(tracks.index[0]).shape == (N_MFCC, 2582)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Summary statistics\n",
    "\n",
    "The `compute_mfcc` function we developed above computes `N_MFCC` coefficients per window in time. Notice that we computed `N_MFCC` x 2582 coefficients for the first track. To have a fixed representation for each complete track (and not for each window of 2048 samples), we need to aggregate those coefficients along time. We'll do it with 7 summary statistics: the minimum, the maximum, the median, and the first 4 moments, i.e. the mean, the standard deviation, the skew and the kurtosis.\n",
    "\n",
    "Below, we construct the DataFrame that will hold our features. Note the use of a hierarchical index. Because I'm a nice guy, I computed the features for most tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('../data/fma_features.csv', index_col=0, header=[0, 1, 2])\n",
    "assert (tracks.index == features.index).all()\n",
    "\n",
    "features.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though I forgot to compute them for the first 10 tracks. ;-) Complete the below code to do that.\n",
    "\n",
    "Hints:\n",
    "* Functions to compute the mentioned statistics are found in `np` and `stats` (from `scipy`, see imports).\n",
    "* We use `tqdm` to show progress on long computations. For example: `for i in tqdm(range(10)): print(i)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tid in tqdm(tracks.index[:10]):\n",
    "    mfcc = compute_mfcc(tid)\n",
    "    features.at[tid, ('mfcc', 'mean')] = np.mean(mfcc, axis=1)\n",
    "    features.at[tid, ('mfcc', 'std')] = np.std(mfcc, axis=1)\n",
    "    features.at[tid, ('mfcc', 'skew')] = stats.skew(mfcc, axis=1)\n",
    "    features.at[tid, ('mfcc', 'kurtosis')] = stats.kurtosis(mfcc, axis=1)\n",
    "    features.at[tid, ('mfcc', 'median')] = np.median(mfcc, axis=1)\n",
    "    features.at[tid, ('mfcc', 'min')] = np.min(mfcc, axis=1)\n",
    "    features.at[tid, ('mfcc', 'max')] = np.max(mfcc, axis=1)\n",
    "\n",
    "features['mfcc','mean'].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Feature selection\n",
    "\n",
    "Once you are done with the rest of the assignment and reach section 5.4, come back here and try to identify the most relevant features for our end goal, i.e. data visualization and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.drop(('mfcc', 'kurtosis'), axis=1, inplace=True)\n",
    "features.drop(('mfcc', 'skew'), axis=1, inplace=True)\n",
    "\n",
    "features.drop(('mfcc', 'min'), axis=1, inplace=True)\n",
    "features.drop(('mfcc', 'max'), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Feature normalization\n",
    "\n",
    "Most algorithms expect (or work better) if the data is centered and standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features -= features.mean(axis=0)\n",
    "features /= features.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Graph construction\n",
    "\n",
    "As opposed to social networks, the brain, or a road network, this dataset does not exhibit a natural graph. But we can always build one! In this case, we will build a similarity graph between tracks. In such a graph, each track is represented as a node and the weight of an edge will be an indication of how similar two tracks are (1 meaning identical, and 0, i.e. no edge, meaning very different). Such graphs are useful for e.g. recommendation. If 10 tracks you liked are strongly connected to an 11th one, you'll probably like that one too (if the similarity measure is relevant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Compute distances\n",
    "\n",
    "Metric\n",
    "\n",
    "The Euclidean, or l2, distance is defined as $$d(i,j) = \\|x_i - x_j\\|_2$$\n",
    "\n",
    "Hints:\n",
    "* Save yourself some pain and use `pdist` and `squareform` from `scipy.spatial.distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = spatial.distance.pdist(features, metric='euclidean')\n",
    "distances = spatial.distance.squareform(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distances.reshape(-1), bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are some distances equal to zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} distances equal exactly zero.'.format(np.sum(distances == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compute the weight matrix\n",
    "\n",
    "Gaussian kernel $$\\mathbf{W}(i,j) = \\exp \\left( \\frac{-d^2(i, j)}{\\sigma^2} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_width = distances.mean()\n",
    "weights = np.exp(-distances**2 / kernel_width**2)\n",
    "\n",
    "np.fill_diagonal(weights, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of graph is that? Fully connected.\n",
    "\n",
    "Sparsify the graph. Either knn or $\\epsilon$. knn better to enforce connectedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(2, 2, figsize=(17, 8))\n",
    "def plot(weights, axes):\n",
    "    axes[0].spy(weights)\n",
    "    axes[1].hist(weights[weights > 0].reshape(-1), bins=50);\n",
    "plot(weights, axes[:, 0])\n",
    "\n",
    "if False:\n",
    "    epsilon = np.percentile(weights, 80)\n",
    "    weights[weights < epsilon] = 0\n",
    "else:\n",
    "    NEIGHBORS = 10\n",
    "    idx = np.argsort(weights)[:, :-NEIGHBORS]\n",
    "    for i in range(weights.shape[0]):\n",
    "        weights[i, idx[i, :]] = 0\n",
    "    weights = np.maximum(weights, weights.T)\n",
    "\n",
    "plot(weights, axes[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Compute the Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = weights.sum(0)\n",
    "\n",
    "plt.hist(degrees, bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shall we use the un-normalized or normalized Laplacian? Choose and justify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinatorial Laplacian.\n",
    "laplacian = np.diag(degrees) - weights\n",
    "\n",
    "# Normalized Laplacian.\n",
    "deg_inv = np.diag(1 / np.sqrt(degrees))\n",
    "laplacian = deg_inv @ laplacian @ deg_inv\n",
    "\n",
    "# Alternatively:\n",
    "# laplacian = np.identity(weights.shape[0]) - deg_inv @ weights @ deg_inv\n",
    "\n",
    "plt.spy(laplacian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian = sparse.csr_matrix(laplacian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many edges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} edges out of {} x {} = {}'.format(laplacian.nnz, *weights.shape, weights.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Bonus\n",
    "\n",
    "Can you think of a way to observe if the two genres form clusters in the graph we created?\n",
    "\n",
    "Hint: Use only the weight matrix / laplacian and the labels.\n",
    "\n",
    "Sort the rows and columns given the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Eigendecomposition of the graph Laplacian\n",
    "\n",
    "No need to compute the Fourier basis, only the Fiedler vector, i.e. the eigenvector associated to $\\lambda_2$.\n",
    "\n",
    "Use one of the following functions: `np.linalg.eig`, `np.linalg.eigh`, `sparse.linalg.eigs`, `sparse.linalg.eigsh`. Justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = sparse.linalg.eigsh(laplacian, k=10, which='SM')\n",
    "\n",
    "# That's much slower:\n",
    "# eigenvalues, eigenvectors = np.linalg.eigh(laplacian.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eigenvalues, '.-', markersize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Connectedness\n",
    "\n",
    "Is the graph connected? Justify. Knowing how we built the graph, can we ensure it is connected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eigenvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.** The graph is connected because only one eigenvalue equals zero (multiplicity one). The graph is connected by construction because of kNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Eigenvector question\n",
    "\n",
    "What do you expect as the result of the below computation? Justify. Do you get the value you expected? If not, why?\n",
    "\n",
    "Note that `x @ y` (introduced in Python 3.5) is equivalent to `np.matmul(x, y)`. You should prefer the former as it makes it easier to read formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(laplacian @ (2 * eigenvectors[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.** We expect zero because the first eigenvalue is zero. As such, the first eigenvector lies in the null-space of the Laplacian. The small error is due to numerical precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Visualization and clustering\n",
    "\n",
    "Finally, let's use the data and graph we prepared. When [exploring data](https://en.wikipedia.org/wiki/Exploratory_data_analysis), it's often useful to visualize an entire dataset. Because for us humans it's hard to look at data in 140 dimensions, we need to somehow reduce the dimensionality to 2 or 3 and visualize the data in this more familiar space. While such a reduction will obviously be destructive, many algorithms have been developed to preserve certain properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Principal component analysis (PCA)\n",
    "\n",
    "[PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) is a standard algorithm to reduce the dimensionality of a dataset. It computes the axes of principal variance and project the data on them. It does not use any graph. We show it here for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pca = decomposition.PCA(n_components=2).fit_transform(features)\n",
    "genres = preprocessing.LabelEncoder().fit_transform(tracks['genre'])\n",
    "plt.scatter(features_pca[:,0], features_pca[:,1], c=genres, cmap='RdBu', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Graph embedding\n",
    "\n",
    "Note how this plot summarizes well 2GB of data and 2000 tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(eigenvectors[:, 1], eigenvectors[:, 2], c=genres, cmap='RdBu', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Clustering\n",
    "\n",
    "Note how we did not try to build a machine to recognize the musical genre given a track (that would have been a [classification problem](https://en.wikipedia.org/wiki/Statistical_classification)). We did merely try to visualize the data, by means of PCA and a graph embedding algorithm. What does it tell us that genre clearly appears in our visualization?\n",
    "\n",
    "**Your answer here.** Genres are useful to categorize songs and organize music collections. Or, the chosen features are (overly?) sensitive to musical genre. Beware of the pitfalls!\n",
    "\n",
    "As such, we can try to cluster the tracks with the Fiedler vector, and look if the (unsupervised) clustering agrees with the *ground truth* genre categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (eigenvectors[:, 1] > 0)\n",
    "\n",
    "plt.scatter(eigenvectors[:, 1], eigenvectors[:, 2], c=labels, cmap='RdBu', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Error rate\n",
    "\n",
    "How many tracks were wrongly identified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.sum(np.abs(labels - genres))\n",
    "err = err if err < len(labels)/2 else len(labels) - err\n",
    "print('{} errors ({:.2%})'.format(err, err/len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune some parameters (e.g. `kernel_width`, `NEIGHBORS`) and discard some features (see section 2.4) to get less errors. You should get an error rate below 15% (i.e. less than 300 errors in total). Try to understand the effect of each parameter. Be aware that tuning the parameters on a specific dataset will lead to [overfitting](https://en.wikipedia.org/wiki/Overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Conclusion\n",
    "\n",
    "Among other things, this assignment showed us that a graph can be useful for e.g. visualization or clustering, even when there is none in the original data. We exercised here two steps of the Data Science process: i) data collection, and ii) data exploitation. The exploitation of the data showed us that a machine can discern musical genres by looking at pairwise distances between spectral features extracted from audio recordings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Bonus\n",
    "\n",
    "What is the name of the technique we used to visualize the data in the last two plots? What does it try to preserve when reducing the dimensionality (of the ambiant space) from 140 to 2?\n",
    "\n",
    "**Your answer here.** The technique is named Laplacian eigenmaps and has been introduced by Belkin and Niyogi in their paper [Laplacian Eigenmaps for Dimensionality Reduction and Data](http://web.cse.ohio-state.edu/~belkin.8/papers/LEM_NC_03.pdf). The algorithm tries to preserve the distance between data samples."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
