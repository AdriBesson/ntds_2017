{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTDS assignment 3: spectral graph theory\n",
    "[MichaÃ«l Defferrard](http://deff.ch), *PhD student*, [EPFL](http://epfl.ch) [LTS2](http://lts2.epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two assignments were designed to warm you up. This third assignment is closer to what you'll have to do for the projects. It only misses the exploratory data analysis part (we'll do that later as an exercise). As such, this exercises is composed of two parts:\n",
    "1. Data collection,\n",
    "2. Data exploitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data collection\n",
    "\n",
    "Like in any data project, the first part of the assignment is to collect some data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the genre of a single track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As often, we need an API key for certain operations. Add the following to your `credentials.ini` file. I gave a key during November 6th lab. If you were not there, ask one of your colleague.\n",
    "```\n",
    "[freemusicarchive]\n",
    "api_key = MY-KEY\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the confidential api key.\n",
    "credentials = configparser.ConfigParser()\n",
    "credentials.read(os.path.join('..', 'credentials.ini'))\n",
    "api_key = credentials.get('freemusicarchive', 'api_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to develop a function to retrieve the genre ID of a track given its track ID using the [FMA API](https://freemusicarchive.org/api).\n",
    "\n",
    "Hints:\n",
    "* A track might have multiple genres associated to it. Always return the first one and discard the others. Note: you should never discard data blindly. I selected the tracks so that this is not a problem.\n",
    "* The `get_genre` function takes an integer as input, the track ID, and returns another integer, the genre ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genre(track_id):\n",
    "    \"\"\"Returns the genre of a track by querying the API.\"\"\"\n",
    "    BASE_URL = 'https://freemusicarchive.org/api/get/tracks.json'\n",
    "    url = '{}?track_id={}&api_key={}'.format(BASE_URL, track_id, api_key)\n",
    "    response = requests.get(url).json()\n",
    "    return int(response['dataset'][0]['track_genres'][0]['genre_id'])\n",
    "\n",
    "# A correct implementation should pass the below test.\n",
    "assert get_genre(1219) == 38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create a table of tracks\n",
    "\n",
    "The `fma_tracks.csv` file contains a list of 2'000 tracks that we will use through this assignment. Each track belongs to either one of the following *top-level genres*: Electronic (`genre_id=12`) and Hip-Hop (`genre_id=21`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv('../data/fma_tracks.csv', index_col=0)\n",
    "print('You got {} track IDs.'.format(len(tracks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once imported by pandas, each row of the DataFrame represents a track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Add the genre to the table\n",
    "\n",
    "Your task is to add a `genre` column to the above created `tracks` DataFrame. The column should contain an integer which represents the genre of the track, i.e. the return value of the `get_genre` function you developed.\n",
    "\n",
    "Hints:\n",
    "* When developing, retrieve the genre of the first 10 tracks only. Only once your code works run it through all the tracks. That will save you time.\n",
    "* As we want to apply one function (`get_genre`) to many data samples (the 2000 track IDs), use a functional approach. Check out `tracks.apply()`.\n",
    "* In Python, you can declare an [anonymous function](https://en.wikipedia.org/wiki/Anonymous_function) as `lambda x: x + 1`.\n",
    "* Your table should look like the below table, except with the correct number instead of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks['genre'] = 0\n",
    "tracks.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = tracks[:3]\n",
    "\n",
    "tracks['genre'] = tracks.apply(lambda track: get_genre(track.name), axis=1)\n",
    "\n",
    "# Alternatively:\n",
    "# tracks['genre'] = list(map(get_genre, tracks.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Save the data\n",
    "\n",
    "To avoid having to collect the data everytime you restart the kernel, save the DataFrame as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tracks.to_csv('../data/fma_tracks_with_genre.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now load it back with the following call instead of running the code in sections 1.1 to 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv('../data/fma_tracks_with_genre.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Data cleaning\n",
    "\n",
    "As always, data cleaning is necessary when dealing with real (as opposed to synthetic) data. In this case, we only need to \"summarize the genres\". The tracks I've selected for the assignment are only part of the Electronic and Hip-Hop top genres. There *actual genre(s)* might however be more specific and be a sub-genre of those too. E.g. IDM is a sub-genre of Electronic. You can explore the genre hierarchy on the [Free Music Archive](http://freemusicarchive.org/genre/Electronic/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tracks_m.loc[tracks.index, ('track', 'genres')].apply(lambda x: x[0]).unique()\n",
    "#tracks['genre'] = tracks_m.loc[tracks.index, ('track', 'genres')].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_genre(genre_id):\n",
    "    return 21 if genre_id in [21, 83, 100, 539, 542, 811] else 12\n",
    "\n",
    "tracks = tracks.applymap(lambda genre: get_top_genre(genre))\n",
    "tracks.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went fine, you should now have 1000 Rock (`genre_id=12`) and 1000 Hip-Hop (`genre_id=12`) tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Feature extraction\n",
    "\n",
    "As is often the case, the data at hand is too large to be dealt with directly. We have to represent it with a smaller set of features, chosen to be maximally relevant to the task. (Manual feature extraction can sometimes be replaced by end-to-end learning systems.)\n",
    "\n",
    "For music, MFCC are often relevant spectral features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen to the music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('../data/fma_features.csv', index_col=0, header=[0, 1, 2])\n",
    "features = features.loc[tracks.index, 'mfcc']\n",
    "assert (tracks.index == features.index).all()\n",
    "\n",
    "features.shape, tracks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features -= features.mean(0)\n",
    "features /= features.std(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Graph construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is the graph connected?\n",
    "* Shall we use the un-normalized or normalized Laplacian? Choose and justify.\n",
    "\n",
    "Compute the l2. Or choose.\n",
    "\n",
    "Hints:\n",
    "* Use the `distance.pdist()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Compute distances\n",
    "\n",
    "Metric\n",
    "\n",
    "The Euclidean distance is defined as $$d(i,j) = \\|x_i - x_j\\|_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "distances = distance.pdist(features, metric='euclidean')\n",
    "distances = distance.squareform(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distances.reshape(-1), bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are some distances equal to zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} distances equal exactly zero. Why?'.format(np.sum(distances == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compute the weight matrix\n",
    "\n",
    "Gaussian kernel $$\\mathbf{W}(i,j) = \\exp \\left( \\frac{-d^2(i, j)}{\\sigma^2} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_width = distances.mean()\n",
    "weights = np.exp(-distances**2 / kernel_width**2)\n",
    "\n",
    "np.fill_diagonal(weights, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of graph is that? Fully connected.\n",
    "\n",
    "Sparsify the graph. Either knn or $\\epsilon$. knn better to enforce connectedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(2, 2, figsize=(17, 8))\n",
    "def plot(weights, axes):\n",
    "    axes[0].spy(weights)\n",
    "    axes[1].hist(weights[weights > 0].reshape(-1), bins=50);\n",
    "plot(weights, axes[:, 0])\n",
    "\n",
    "if False:\n",
    "    epsilon = np.percentile(weights, 80)\n",
    "    weights[weights < epsilon] = 0\n",
    "else:\n",
    "    NEIGHBORS = 10\n",
    "    idx = np.argsort(weights)[:, :-NEIGHBORS]\n",
    "    for i in range(weights.shape[0]):\n",
    "        weights[i, idx[i, :]] = 0\n",
    "    weights = np.maximum(weights, weights.T)\n",
    "\n",
    "plot(weights, axes[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Compute the Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = weights.sum(0)\n",
    "\n",
    "plt.hist(degrees, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinatorial Laplacian.\n",
    "laplacian = np.diag(degrees) - weights\n",
    "\n",
    "# Normalized Laplacian.\n",
    "deg_inv = np.diag(1 / np.sqrt(degrees))\n",
    "laplacian = deg_inv @ laplacian @ deg_inv\n",
    "\n",
    "# Alternatively:\n",
    "# laplacian = np.identity(weights.shape[0]) - deg_inv @ weights @ deg_inv\n",
    "\n",
    "plt.spy(laplacian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian = sparse.csr_matrix(laplacian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many edges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} edges out of {} x {} = {}'.format(laplacian.nnz, *weights.shape, weights.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Bonus\n",
    "\n",
    "Can you think of a way to observe if the two genres form clusters in the graph we created?\n",
    "\n",
    "Hint: Use only the weight matrix / laplacian and the labels.\n",
    "\n",
    "Sort the rows and columns given the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Eigenvectors & eigenvalues\n",
    "\n",
    "No need to compute the Fourier basis, only the Fiedler vector, i.e. the eigenvector associated to $\\lambda_2$.\n",
    "\n",
    "Use one of the following functions: `np.linalg.eig`, `np.linalg.eigh`, `sparse.linalg.eigs`, `sparse.linalg.eigsh`. Justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = sparse.linalg.eigsh(laplacian, k=10, which='SM')\n",
    "\n",
    "# That's much slower:\n",
    "# eigenvalues, eigenvectors = np.linalg.eigh(laplacian.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eigenvalues, '.-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the graph connected? Justify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you expect as the result of the below computation? Justify. Do you get the value you expected? If not, why?\n",
    "\n",
    "Note that `x @ y` is equivalent to `np.matmul(x, y)`. You should prefer the former as it makes it easier to read formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(laplacian @ eigenvectors[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.** We expect zero because the first eigenvalue is zero. The small error is due to numerical precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare different techniques.\n",
    "PCA, Fiedler, spectral clustering\n",
    "\n",
    "Visualization with Laplacian eigenmaps\n",
    "\n",
    "Principal component analysis (PCA), no graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition\n",
    "\n",
    "features_pca = skl.decomposition.PCA(n_components=2).fit_transform(features)\n",
    "\n",
    "genres = skl.preprocessing.LabelEncoder().fit_transform(tracks)\n",
    "\n",
    "plt.scatter(features_pca[:,0], features_pca[:,1], c=genres, cmap='RdBu', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(eigenvectors[:, 1], eigenvectors[:, 2], c=genres, cmap='RdBu', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the tracks with the Fiedler vector. How many tracks were wrongly identified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (eigenvectors[:, 1] > 0)\n",
    "\n",
    "plt.scatter(eigenvectors[:, 1], eigenvectors[:, 2], c=labels, cmap='RdBu', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.sum(np.abs(labels - genres))\n",
    "err = err if err < len(labels)/2 else len(labels) - err\n",
    "print('{} errors ({}%)'.format(err, err/len(labels)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune some parameters (e.g. `kernel_width`, `NEIGHBORS`) to get less errors. You should get an error rate lower than 15% (i.e. less than 300 errors in total). Try to understand the effect of each parameter."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
